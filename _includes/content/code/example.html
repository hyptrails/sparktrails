<p>
	Let us consider four states <em>a,b,c,d</em>,
	an observation matrix \(D\) 
	and two hypothesis matrices \(H1\) and \(H2\).
	The observation matrix contains the transition counts \(d_{i,j}\) between states.
	For example in our case we observed six transitions from state \(a\) to state \(b\), 
	i.e., \(d_{a,b} = 6\).
	The hypothesis matrices \(H_i\) contain transition probabilities \(p_{i,j}\) between states.
	For example in our case, the user hypothesis \(H_1\) assumes that the probability 
	for a user at state \(c\) to move to state \(d\) is 0.8, 
	i.e., \(p_{c,d} = 0.8\).
</p>
$$
	D =
	\left(
	\begin{matrix}
1 & 6 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 3 & 5 \\
5 & 0 & 2 & 4 
	\end{matrix}
	\right)
\hspace{3cm}
	H_1 = 
	\left(
	\begin{matrix}
0.1 & 0.9 & 0.0 & 0.0 \\
0.0 & 1.0 & 0.9 & 0.0 \\
0.0 & 0.1 & 0.1 & 0.8 \\
0.1 & 0.1 & 0.1 & 0.7 
	\end{matrix}
	\right)
$$

<p>
Given these observations as well as the user hypotheses \(H_1\) and \(H_2\), 
the HypTrails approach calculates log-evidences \(log(P_\kappa(D|H))\)
for several believe strengths \(\kappa\).
</p>

<p>
	In our example we save these matrices in a Hadoop file system (HDFS).
	We encode them in a sparse row format, where
	zero entries are left out.
	Each row starts with a row number followed by a tab 
	and a comma separated list of tuples <code>x;y</code>.
	Each tuple is made of a column number 
	<code>x</code> and a column value <code>y</code>.
</p>

<p>
	We save \(D\) under <code>hdfs://data/observations.row</code> which looks like this:
</p>
<pre>
0	0;1,1;6
2	2;3,3;5
3	0;5,2;2,3;4
</pre>

<p>
	We save \(H_1\) under <code>hdfs://data/hypothesis1.row</code> 
	and \(H_2\) under <code>hdfs://data/hypothesis2.row</code>.
	\(H_1\) looks like this:
</p>
<pre>
0	0;0.1,1;0.9
1	1;1.0,2;0.9
2	1;0.1,2;0.1,3;0.8
3	0;0.1,1;0.1,2;0.1,3;0.7 
</pre>